{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Nsoh7QP8qRbq",
      "metadata": {
        "id": "Nsoh7QP8qRbq"
      },
      "source": [
        "# Copy of data exporting an proccessing Code\n",
        "****\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "88aav2D8qZpj",
      "metadata": {
        "id": "88aav2D8qZpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad85eb77-2227-4108-f713-3e4dca00ae5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n",
            "There are 20 different categories, which are:\n",
            "['BearHead', 'CatHead', 'ChickenHead', 'CowHead', 'DeerHead', 'DogHead', 'DuckHead', 'EagleHead', 'ElephantHead', 'HumanHead', 'LionHead', 'MonkeyHead', 'MouseHead', 'PandaHead', 'PigHead', 'PigeonHead', 'RabbitHead', 'SheepHead', 'TigerHead', 'WolfHead']\n",
            "\n",
            "\n",
            "The chosen categories for One-Shot Learning are: ['DuckHead', 'WolfHead', 'PigHead', 'LionHead', 'EagleHead']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 502 files [00:00, 4555.86 files/s]\n",
            "Copying files: 1905 files [00:00, 4708.75 files/s]\n"
          ]
        }
      ],
      "source": [
        "! pip install split-folders\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import splitfolders #(pip install split-folders)\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "## Downloading the database\n",
        "\n",
        "# If you do not posess the AnimalFace.zip archive, this cells downloads it\n",
        "\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data/')\n",
        "    \n",
        "if not os.path.exists('data/images/') or len(os.listdir('data/images/')) == 0:\n",
        "    if not os.path.exists('data/AnimalFace.zip'):\n",
        "        url = 'https://vcla.stat.ucla.edu/people/zhangzhang-si/HiT/AnimalFace.zip'\n",
        "        urllib.request.urlretrieve(url, 'data/AnimalFace.zip')\n",
        "    with zipfile.ZipFile('data/AnimalFace.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('data/')\n",
        "    os.rename('data/Image/', 'data/images/')\n",
        "\n",
        "if os.path.exists('./data/images/Natural'):\n",
        "    shutil.rmtree('./data/images/Natural')\n",
        "\n",
        "# Download the zip and unzip in a folder named Images\n",
        "# And we delete the Natural folder which contains only 8 images non related to AnimalFaces\n",
        "\n",
        "## Chosing the categories for One Shot Learning\n",
        "\n",
        "categories = os.listdir('data/images')\n",
        "if os.path.exists('data/OneShotImages/'):\n",
        "    oneshot_cats = sorted(os.listdir('data/OneShotImages/'))\n",
        "    categories += oneshot_cats\n",
        "categories = sorted([cat for cat in categories])\n",
        "print(f'There are {len(categories)} different categories, which are:', categories, '\\n', sep='\\n')\n",
        "\n",
        "if not os.path.exists('data/OneShotImages/'):\n",
        "    np.random.seed(42)\n",
        "    oneshot_cats = np.random.choice(len(categories), 5)\n",
        "    oneshot_cats = [categories[num] for num in oneshot_cats]\n",
        "print('The chosen categories for One-Shot Learning are:', oneshot_cats)\n",
        "\n",
        "train_cats = list(set(categories) - set(oneshot_cats))\n",
        "\n",
        "if not os.path.exists('data/OneShotImages/'):\n",
        "    os.makedirs('data/OneShotImages/')\n",
        "    for cat in oneshot_cats:\n",
        "        shutil.move('data/images/' + cat, 'data/OneShotImages/' + cat)\n",
        "\n",
        "## Separating the folders into train and test\n",
        "\n",
        "IMAGES_PATH = 'data/images/'\n",
        "IMAGES_OS_PATH = 'data/OneShotImages/'\n",
        "\n",
        "splitfolders.fixed(IMAGES_OS_PATH, output='data/', seed=42, fixed=(1, 0, -1), group_prefix=None, move=False)\n",
        "shutil.rmtree('data/val/')\n",
        "os.rename('data/train/', 'data/train_os/')\n",
        "os.rename('data/test/', 'data/test_os/')\n",
        "\n",
        "splitfolders.ratio(IMAGES_PATH, output='data/', seed=42, ratio=(.8, .0, .2), group_prefix=None, move=False)\n",
        "shutil.rmtree('data/val/')\n",
        "\n",
        "## Visualization of the database\n",
        "\n",
        "batch_size = 32\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5LV9zvJgqdHb",
      "metadata": {
        "id": "5LV9zvJgqdHb"
      },
      "source": [
        "# Start of Clip Code\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "90exzNUlqnsL",
      "metadata": {
        "id": "90exzNUlqnsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "662da9c4-6408-4e09-bee9-42b031121af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-sq8c5hbp\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-sq8c5hbp\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.63.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.5)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369221 sha256=b90a6c231848cc03f656413d73d2355b3735358f3a6e0d634e5419475416a347\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4zqmyciy/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5fe1c08a",
      "metadata": {
        "id": "5fe1c08a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import clip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QjRK2F8Zrh51",
      "metadata": {
        "id": "QjRK2F8Zrh51"
      },
      "source": [
        "## Training utils functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "4c5b0f7b",
      "metadata": {
        "id": "4c5b0f7b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train_os', 'test_os']:\n",
        "            if phase == 'train_os':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train_os'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train_os':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test_os' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'test_os':\n",
        "                val_acc_history.append(epoch_acc.item())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "1aac2cf8",
      "metadata": {
        "id": "1aac2cf8"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "cJ3Cq_hCsE5D",
      "metadata": {
        "id": "cJ3Cq_hCsE5D"
      },
      "outputs": [],
      "source": [
        "class model_classif(torch.nn.Module):\n",
        "    def __init__(self,model,n_out,feature_extraction=True):\n",
        "        super(model_classif, self).__init__()\n",
        "        self.model_pretrained = model # The pretrained model\n",
        "\n",
        "        set_parameter_requires_grad(self.model_pretrained,feature_extraction)\n",
        "        self.fc = nn.Linear(self.model_pretrained.output_dim,n_out)  # The output layer we add\n",
        "        self.gle = nn.GELU()  # The activation function\n",
        "\n",
        "    def forward(self,x):\n",
        "        res0=self.model_pretrained(x)\n",
        "        res1=self.fc(res0)\n",
        "        res2=self.gle(res1)\n",
        "\n",
        "        return res2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tIwDHHL0r40E",
      "metadata": {
        "id": "tIwDHHL0r40E"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cd3c711c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd3c711c",
        "outputId": "52230779-e8ae-49c1-9632-4e69e9f37db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 148MiB/s]\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "ed61bf26",
      "metadata": {
        "id": "ed61bf26"
      },
      "outputs": [],
      "source": [
        "# Number of classes in the dataset\n",
        "num_classes = 5\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 5\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 1\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "\n",
        "data_dir = './data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "e3001c88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3001c88",
        "outputId": "1f70f132-3421-4f48-d796-928f1bed3d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), preprocess) for x in ['train_os', 'test_os']}\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train_os', 'test_os']}\n",
        "\n",
        "dataloaders_dict = {\n",
        "    'train_os': torch.utils.data.DataLoader(image_datasets[\"train_os\"], batch_size=batch_size, shuffle=False, num_workers=4), \n",
        "    'test_os': torch.utils.data.DataLoader(image_datasets[\"test_os\"], batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Detect if the GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m-cvkE6GrMaZ",
      "metadata": {
        "id": "m-cvkE6GrMaZ"
      },
      "source": [
        "# One Fine-tuned Linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "65819416",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65819416",
        "outputId": "b55e154d-4b76-4c24-c720-3de84a733c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "feature_extract = True\n",
        "\n",
        "model_feat_extr_1 = model_classif(model.visual,n_out=num_classes,feature_extraction=feature_extract)\n",
        "input_size = model.visual.input_resolution\n",
        "\n",
        "\n",
        "\n",
        "model_feat_extr_1 = model_feat_extr_1.to(device).float()\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_feat_extr_1.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_feat_extr_1.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_feat_extr_1.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_feat_extr_1 = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "a68ba017",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68ba017",
        "outputId": "e2acf628-6b58-435a-dd8f-1c55fda5cf3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/0\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_os Loss: 1.6468 Acc: 0.2000\n",
            "test_os Loss: 1.6608 Acc: 0.1952\n",
            "\n",
            "Training complete in 0m 8s\n",
            "Best val Acc: 0.195171\n"
          ]
        }
      ],
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_feat_extr_1, hist_feat_extr_1 = train_model(model_feat_extr_1, dataloaders_dict, criterion, optimizer_feat_extr_1, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "666532a4",
      "metadata": {
        "id": "666532a4"
      },
      "source": [
        "# One shot on image to image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "data_dir = './data/'\n",
        "\n",
        "image_datasets_one_shot = datasets.ImageFolder(os.path.join(data_dir, \"test_os\"), preprocess)\n",
        "dataloaders_one_shot  = torch.utils.data.DataLoader(image_datasets_one_shot, batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "image_datasets_train_one_shot = datasets.ImageFolder(os.path.join(data_dir, \"train_os\"), preprocess)\n",
        "dataloaders_train_one_shot  = torch.utils.data.DataLoader(image_datasets_one_shot, batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "# Detect if the GPU is available\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUJGq9I3lkVQ",
        "outputId": "e550f82f-b792-4e92-e680-660972c61752"
      },
      "id": "TUJGq9I3lkVQ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Datasets and Dataloaders...\n",
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets_train_one_shot = datasets.ImageFolder(os.path.join(data_dir, \"train_os\"), preprocess)\n",
        "dataloaders_train_one_shot  = torch.utils.data.DataLoader(image_datasets_train_one_shot, batch_size=1, shuffle=False, num_workers=4)\n",
        "# Calculate features\n",
        "success = 0\n",
        "nb_tot = 0\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_features = torch.cat([model.encode_image(inputs.to(device)) for inputs, labels in dataloaders_train_one_shot])\n",
        "  train_features /= train_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  for inputs, labels in dataloaders_one_shot:\n",
        "    inputs = inputs.to(device)\n",
        "    image_features = model.encode_image(inputs)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * image_features @ train_features.T).softmax(dim=-1)\n",
        "    values, indices = similarity[0].topk(1)\n",
        "    if indices.item() == labels.item():\n",
        "      success += 1\n",
        "    nb_tot += 1\n",
        "\n",
        "accuracy = success/nb_tot\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8fHi_f2m1tV",
        "outputId": "f5778070-d6ea-4166-f24d-12b19fa21f48"
      },
      "id": "e8fHi_f2m1tV",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9798792756539235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One shot on image to text"
      ],
      "metadata": {
        "id": "Bx6arrSPlYzE"
      },
      "id": "Bx6arrSPlYzE"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "dcda5fee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcda5fee",
        "outputId": "e9c42f5e-a6c5-481b-a55d-73ab6f4a4510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a photo of a the head of duck', 'a photo of a the head of eagle', 'a photo of a the head of lion', 'a photo of a the head of pig', 'a photo of a the head of wolf']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9818913480885312\n"
          ]
        }
      ],
      "source": [
        "# Calculate features\n",
        "\n",
        "import re\n",
        "\n",
        "success = 0\n",
        "nb_tot = 0\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  oneshot_cats.sort()\n",
        "  all_cats = [\"a photo of a the head of {animal}\".format(animal = re.split('Head', c)[0].lower()) for c in oneshot_cats]\n",
        "  text_inputs = torch.cat([clip.tokenize(c) for c in all_cats]).to(device)\n",
        "  text_features = model.encode_text(text_inputs)\n",
        "  text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  for inputs, labels in dataloaders_one_shot:\n",
        "    inputs = inputs.to(device)\n",
        "    image_features = model.encode_image(inputs)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    values, indices = similarity[0].topk(1)\n",
        "    if indices.item() == labels.item():\n",
        "      success += 1\n",
        "    nb_tot += 1\n",
        "\n",
        "accuracy = success/nb_tot\n",
        "print(accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Nsoh7QP8qRbq",
        "5LV9zvJgqdHb",
        "m-cvkE6GrMaZ",
        "666532a4",
        "Bx6arrSPlYzE"
      ],
      "name": "Clip_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}